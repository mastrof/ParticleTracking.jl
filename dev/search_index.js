var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/#Blobs","page":"API","title":"Blobs","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The detect_blobs function processes an image to identify relevant features in the form of blobs, which are represented by the Blob type.","category":"page"},{"location":"api/","page":"API","title":"API","text":"AbstractBlob\nBlob","category":"page"},{"location":"api/#ParticleTracking.AbstractBlob","page":"API","title":"ParticleTracking.AbstractBlob","text":"AbstractBlob{T,S,N}\n\nAbstract interface for blob types\n\n\n\n\n\n","category":"type"},{"location":"api/#ParticleTracking.Blob","page":"API","title":"ParticleTracking.Blob","text":"struct Blob{T,S,N} <: AbstractBlob{T,S,N}\n    location::CartesianIndex{N}\n    œÉ::S\n    amplitude::T\n    m0::T\n    m2::T\n    intensity_map\nend\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API","title":"API","text":"The properties of a Blob can be accessed through the following functions.","category":"page"},{"location":"api/","page":"API","title":"API","text":"location\nlocation_raw\nscale\namplitude\nradius\nzeroth_moment\nsecond_moment\nintensity_map","category":"page"},{"location":"api/#ParticleTracking.location","page":"API","title":"ParticleTracking.location","text":"location(blob)\n\nReturn the estimated location of the blob including subpixel refinement. The raw location can be obtained with location_raw.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.scale","page":"API","title":"ParticleTracking.scale","text":"scale(blob)\n\nReturn the estimated scale of the blob along each dimension.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.amplitude","page":"API","title":"ParticleTracking.amplitude","text":"amplitude(blob)\n\nReturn the peak amplitude of the blob.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.radius","page":"API","title":"ParticleTracking.radius","text":"radius(blob)\n\nReturn the equivalent spherical radius of the blob.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.zeroth_moment","page":"API","title":"ParticleTracking.zeroth_moment","text":"zeroth_moment(blob)\n\nReturn the zeroth intensity moment of the blob image.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.second_moment","page":"API","title":"ParticleTracking.second_moment","text":"second_moment(blob)\n\nReturn the second intensity moment of the blob image.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.intensity_map","page":"API","title":"ParticleTracking.intensity_map","text":"intensity_map(blob)\n\nReturn the image of the blob.\n\n\n\n\n\n","category":"function"},{"location":"api/#Cost","page":"API","title":"Cost functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"blobtracking links blob into trajectories by finding the optimal solution to the minimum cost flow problem defined by the keyword cost function. Two cost functions are provided out of the box:","category":"page"},{"location":"api/","page":"API","title":"API","text":"QuadraticCost\nPCost","category":"page"},{"location":"api/#ParticleTracking.QuadraticCost","page":"API","title":"ParticleTracking.QuadraticCost","text":"QuadraticCost(A, B; g0=1.0, g2=1.0)\n\nEvaluate the cost of linking two blobs A and B using the Euclidean (ùêø¬≤) norm.\n\nThe cost includes the spatial distance of the two blobs, and the distance in their zeroth and second intensity moments.\n\nKeywords\n\ng0::Real = 1.0: weight factor for the zeroth moment\ng2::Real = 1.0: weight factor for the second moment\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.PCost","page":"API","title":"ParticleTracking.PCost","text":"PCost(A, B; p=1, g0=1.0, g2=1.0)\n\nEvaluate the cost of linking two blobs A and B using the ùêø·µñ norm.\n\nThe cost includes the spatial distance of the two blobs, and the distance in their zeroth and second intensity moments.\n\nKeywords\n\np::Real = 1: the norm exponent; p=1 corresponds to the Manhattan metric, p=Inf to the Chebyshev metric, and p=2 to the Euclidean metric (in which case QuadraticCost provides slightly better performance)\ng0::Real = 1.0: weight factor for the zeroth moment\ng2::Real = 1.0: weight factor for the second moment\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API","title":"API","text":"However, arbitrary cost functions can be defined by the user, and they will \"just work\" as long as they respect the API. A cost function must:","category":"page"},{"location":"api/","page":"API","title":"API","text":"take two and only two positional arguments which are subtypes of AbstractBlob\naccept any other relevant argument in the form of a keyword argument\nnot modify the state of the blobs\nalways return a non-negative real number","category":"page"},{"location":"api/","page":"API","title":"API","text":"Cost functions do not have to explicitly take care of maximum allowed distances, they should only evaluate the cost of linking the two input blobs.","category":"page"},{"location":"api/#Trajectories","page":"API","title":"Trajectories","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"The result of the blobtracking is a vector of Trajectory objects.","category":"page"},{"location":"api/","page":"API","title":"API","text":"Trajectory","category":"page"},{"location":"api/","page":"API","title":"API","text":"A Trajectory contains a vector of blobs and an associated vector of time points, which are not necessarily contiguous (in fact, when tracking with memory>1, we allow a trajectory to contain gaps).","category":"page"},{"location":"api/","page":"API","title":"API","text":"All the functions of the Blob API can be also called on Trajectory objects, and, they will return a vector with the value of the requested property at each step along the trajectory.","category":"page"},{"location":"api/","page":"API","title":"API","text":"A Trajectory also supports the following functions","category":"page"},{"location":"api/","page":"API","title":"API","text":"length\ntimestamps\nstartpoint\nendpoint","category":"page"},{"location":"api/#Base.length","page":"API","title":"Base.length","text":"length(trajectory)\n\nReturn the length of the trajectory.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.timestamps","page":"API","title":"ParticleTracking.timestamps","text":"timestamps(trajectory)\n\nReturn the times along the trajectory at which blobs have been measured.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.startpoint","page":"API","title":"ParticleTracking.startpoint","text":"startpoint(trajectory)\n\nReturn the time at which the trajectory started.\n\n\n\n\n\n","category":"function"},{"location":"api/#ParticleTracking.endpoint","page":"API","title":"ParticleTracking.endpoint","text":"endpoint(trajectory)\n\nReturn the time at which the trajectory ended.\n\n\n\n\n\n","category":"function"},{"location":"#ParticleTracking.jl","page":"Home","title":"ParticleTracking.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Detection and tracking of blob-like objects in Julia.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The goal of this package is to provide a pure Julia alternative to libraries available in other languages and frameworks (e.g. trackpy, TrackMate), aiming at speed and usability at least for simple tracking problems.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Blob detection based on Laplacian of Gaussian filtering\nSubpixel localization\nMin-cost flow tracking with custom cost functions\nConvenient plot recipes and interactive GUIs via Makie.jl","category":"page"},{"location":"#Acknowledgements","page":"Home","title":"Acknowledgements","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk≈Çodowska-Curie grant agreement No 955910.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"EditURL = \"tutorial.jl\"","category":"page"},{"location":"tutorial/#Tutorial","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We will use a sample video to test detection and tracking. The video is available here: [xxx]; we will load it with the Download.jl package. We will walk through the entire pipeline, showcasing most of the package features and how to use them to get the best tracking results.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"It is already preprocessed. For convenience, we want to convert the pixel values to Float16 or Float32 values, and normalize these values in the range [0,1].","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The entire dataset, a three-dimensional array, can be visualized in the form of a volume plot where the x and y axis correspond to the two axes of each image, and the vertical z axis represents time, i.e. the different frames which compose the video. With this plot, we should already be able to identify the salient features of the dataset: small bright objects moving in the xy plane. Our task is to algorithmically select all the points corresponding to \"real\" moving objects, and connect their positions over time.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"At a glance, it should be easy to recognize some straight vertical streaks, corresponding to objects which are barely moving in the xy plane, and then 2 clouds of scattered points, corresponding to objects that are actually moving.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using Downloads, TiffImages, CairoMakie\nfpath = joinpath(pwd(), \"sample_video.tif\")\nDownloads.download(\n    \"https://github.com/mastrof/ParticleTracking.jl/raw/docs/docs/src/sample_video.tif\",\n    fpath\n)\nvideo = TiffImages.load(fpath) .|> Float16\nrm(fpath) ## clean up after loading the video\nvideo .-= minimum(video)\nvideo ./= maximum(video)\nvolume(video)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Before performing the full detection, we can explore the data to find the optimal parameters.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"An interactive GUI is provided through GLMakie with the gui_blobs function. In the large main window we see one frame of the video, with the detected blobs surrounded by red circles. We can manipulate the size of the blobs we want to detect and set an amplitude threshold to filter out spurious low-intensity spots which are wrongly detected as objects. We can also zoom in and move the image around, as well as check the quality of the detection on different frames.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"On the side, we see a scatter plot showing the zeroth (m‚ÇÄ) and second (m‚ÇÇ) intensity moments of the detected blobs (using a batch of 20 frames to provide some meaningful statistics). The values of these moments can be used for further filtering or discrimination.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"gui_blobs(video)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"After exploring, we can set the desired parameters and actually perform the detection. We will limit the blob sizes between 2 to 4 pixels, and set the intensity threshold to 0.05.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The detection is performed with the detect_blobs function, which has to be applied individually to each frame of video.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ParticleTracking\n\nblobsize = 2:4\nrthresh = 0.05\nblobs = [detect_blobs(img, blobsize; rthresh) for img in eachslice(video; dims=3)]","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"blobs is a vector where each element corresponds to a single frame of video. Each element is itself a vector of Blobs, an object which contains all the information about the blob; its position in the image, its size and other properties. The Blob API is described here.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"A typical step after the initial detection is to look at the zeroth and second intensity moments of the blobs, which can provide information about differences between the detected objects.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"m0 = map.(zeroth_moment, blobs)\nm2 = map.(second_moment, blobs)\nscatter(vcat(m0...), vcat(m2...); alpha=0.5, axis=(xlabel=\"m0\", ylabel=\"m2\"))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The zeroth moment (m0) is proportional to the total intensity of the blob; the tight cluster in the bottom left (0.5<m0<0.7 and 1.6<m2<1.7), which is well separated from the rest of the datapoints, is then likely to identify spurious detections resulting from pixel shot noise. If this is the case, it would be good to filter out these blobs before moving on to the tracking. A clean set of blobs without spurious detections can drastically improve the tracking quality, but excessive filtering can have the opposite effects.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"To verify our hypothesis, i.e., low-m0 low-m2 blobs are spurious detections, we have to visually check what these blobs correspond to in the image. We can then separate the two populations and highlight them in the images with different colors.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"maybe_spurious = [\n    findall(@. (0.5 < m0[i] < 0.7) && (1.6 < m2[i] < 1.7))\n    for i in eachindex(blobs)\n]\nA = @. view(blobs, maybe_spurious)\nB = @. view(blobs, setdiff(eachindex(blobs), maybe_spurious))\n\n# visualize the two populations in frame t\nlet t = 1\n    fig, ax, = heatmap(video[:,:,t], colormap=:bone)\n    # blobs can be directly plotted in Makie with scatter\n    scatter!(ax, A[t], color=:red, alpha=0.1, markersize=30)\n    scatter!(ax, B[t], color=:green, alpha=0.1, markersize=30)\n    fig\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The red marker (as can be confirmed by trying different frames t), is always associated to a specific object in the image, but we clearly see it as quite bright! Why is it different from the others then? By zooming in, we can see that it is a single bright pixel, whereas the other blobs, while still extremely small, correspond to a larger bright area.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"It turns out that the camera I used to collect this video has a dead pixel, which is always showing up as illuminated. That is what our moments-based filtering has allowed us to identify.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"It is also possible to directly access the intensity_map of a blob, i.e. the set of pixels which have been identified as a unique blob. We can, for instance, plot the intensity_map of this spurious blob along side that of another blob (j) in the other population. We see that \"real\" blobs are generally brighter and/or more extended than the spurious one.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"let t=1, I_A=intensity_map(A[t][1]), j=1, I_B=intensity_map(B[t][j])\n    M = max(maximum(I_A), maximum(I_B))\n    fig, ax1 = heatmap(I_A; colorrange=(0,M), axis=(title=\"Spurious\",))\n    ax2 = Axis(fig[1,2], title=\"Real\")\n    heatmap!(ax2, I_B; colorrange=(0,M))\n    fig\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Now we can safely filter out the spurious detections and move on to the tracking using the blobtracking function.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In principle, we don't need to do anything special and we can just call blobtracking(blobs), but we will see the results are much less than optimal.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"# reassign blobs to the subpopulation without spurious detections\nblobs = deepcopy(B)\n# do the tracking\ntrajectories = blobtracking(blobs)\n# visualize plotting trajectories one by one\nlet\n    fig = Figure()\n    ax = Axis(fig[1,1])\n    for track in trajectories\n        lines!(ax, track)\n    end\n    fig\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We see that the algorithm tends to connect points that are far away from each other. Indeed, since the tracking is based on finding the \"minimum cost flow\" between successive frames, it always tries to connect points in order to not interrupt any trajectory. This leads, however, to unreasonable connections. The first thing to do, therefore, is to define the maximum distance that an object can cover during a single frame: any connection between points at a distance larger than this threshold will be less favorable than to just interrupt the trajectory.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The maximum distance can be set through the maxdist keyword. In our case, after visual inspection it seems reasonable to set this limit to 60 pixels","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"maxdist = 60\ntrajectories = blobtracking(blobs; maxdist)\nlet\n    fig = Figure()\n    ax = Axis(fig[1,1])\n    for track in trajectories\n        lines!(ax, track)\n    end\n    fig\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This is already much better. But we can do more.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"scatter(length.(trajectories))","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"A quick look at the length of the trajectories we obtained shows that we have a few very long trajectories (basically spanning the entire video) a few medium-length ones and then a bunch of trajectories only 1-frame long.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"A 1-frame trajectory is basically a blob that has been detected but not connected to anything. This can happen for various reasons. If it's spurious detections that survived our filtering but didn't produce any connection, that's perfect, we will just filter these trajectories out. But they may also be real objects which, somehow, were not consistently detected across successive frames and hence did not produce a trajectory.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In fact, our algorithm has been only looking at immediately successive frames, but we can also account for the fact that an object might disappear from the field of view for a few frames, and that we may still be able to recognize it if it appears again.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Connections across frame gaps are controlled by the memory keyword. By default it is set to 1, meaning that only the immediate next frame is investigated for a connection. But increasing its value allows us to close gaps due to objects disappearing from the field of view. Allowing too much memory will, however, have detrimental effects: the objects are allowed to travel maxdist every frame, therefore large memory values will allow connections between objects far away in space and time.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"maxdist = 60\nmemory = 5\ntrajectories = blobtracking(blobs; maxdist, memory)\nlet\n    fig = Figure()\n    ax = Axis(fig[1,1])\n    for track in trajectories\n        lines!(ax, track)\n    end\n    fig\nend","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Compared to the previous memoryless trial, the pink trajectory in the bottom has been extended; the three nearby blue, green, light-blue segments now constitute a single, longer trajectory; the two orange and pink trajectories at the top are now joined into a single one; the brownian trajectories on the left have not been modified since they were already optimal.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"This page was generated using Literate.jl.","category":"page"}]
}
